{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586504d2-8dfb-48f4-a3f3-39f0d261c325",
   "metadata": {},
   "source": [
    "# Calculating relevance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c493825-91d9-4d84-9fd0-8fc23d8d221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import random\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "from datetime import datetime, timedelta\n",
    "from urllib import parse\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from irmetrics.topk import rr\n",
    "matplotlib.rcParams['figure.figsize'] = [16, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640f3a34-5743-4718-800b-fadabcd65297",
   "metadata": {},
   "source": [
    "Set some useful parameters. It's important to set a custom User-agent here as we want to be able to exclude these searches from future relevance judgment sets when running against production. Running 20 bootstraps of 250 queries took about 25 minutes for me, adjust this to taste!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8c8460-f858-44a3-a058-45e1489f91a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERP = \"https://caselaw.nationalarchives.gov.uk/judgments/search\"\n",
    "USER_AGENT = \"Tim-MetricsBot/0.0.1\"\n",
    "TIMEOUT = 0.2\n",
    "BOOTSTRAPS = 20\n",
    "SAMPLE_SIZE = 250\n",
    "RESULT_SELECTOR = \".judgment-listing__title\"\n",
    "DEFAULT_PER_PAGE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0cb9da-9347-4eab-8487-f6d17272e9f1",
   "metadata": {},
   "source": [
    "Load the data file we created in the 'Parse cloudfront' notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5168f8c-e481-42d2-a0d1-f49f2b533e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/search_queries_and_documents.json\") as file:\n",
    "    searches = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0819c4-839f-4ebe-831e-7d93837720ea",
   "metadata": {},
   "source": [
    "Define a method to randomly sample from our queries and relevance judgments - bootstraps are sampled with replacement, but the queries within each one  are sampled without - each query within a bootstrap is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b2e498-44fe-4167-9793-47c2404c2f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_searches(searches, n_bootstraps=BOOTSTRAPS, sample_size=SAMPLE_SIZE):\n",
    "    bootstraps = []\n",
    "    for i in range(0, n_bootstraps):\n",
    "        bootstraps.append(random.choices(searches, k=sample_size))\n",
    "    return bootstraps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b392b693-a99f-451b-a60c-7731c8345839",
   "metadata": {},
   "source": [
    "Define a method to do the business of running the search, and returning the urls of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5857a16-28e8-4b03-bf70-4b06936fae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_for_search(search):\n",
    "    search[\"query\"][\"query\"] = parse.unquote(search[\"query\"][\"query\"][0]) # TODO this should be done in creating the judgments file, not here\n",
    "    query_string = parse.urlencode(search[\"query\"], doseq=True)\n",
    "    response = requests.get(\"%s?%s\" % (SERP, query_string), headers={'User-Agent': USER_AGENT})\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    result_elements = soup.select(RESULT_SELECTOR)\n",
    "    urls = [e.find(\"a\", href=True)[\"href\"].split(\"?\")[0] for e in result_elements]\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a23c83-1e83-4a05-bdb3-1227337fa366",
   "metadata": {},
   "source": [
    "Define a method to compute the mean reciprocal rank for a list of 'true positives' and a corresponding list of search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306afecc-b147-48c0-ba2d-58a7ce347b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reciprocal_rank(search_results):\n",
    "    trues = [r[\"true\"] for r in search_results]\n",
    "    returneds = [r[\"returned\"] for r in search_results]\n",
    "    longest = max(len(r) for r in returneds)\n",
    "    returneds_padded = [r + [np.nan] * (longest - len(r)) for r in returneds]\n",
    "    return rr(trues, np.vstack(returneds_padded)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feb6bc5-2884-44a8-8d94-86f8bb73e63f",
   "metadata": {},
   "source": [
    "Define a method to summarise the results across each bootstrap - returning the average MRR, std of MRR, the average rank of the true result and its confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fe9523-9db2-41b2-af56-008eb29c512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_results(results): \n",
    "    mrrs = [r[\"mrr\"] for r in results]\n",
    "    avg_ranks = [1/mrr for mrr in mrrs]\n",
    "    ci = st.t.interval(0.95, len(avg_ranks)-1, loc=np.mean(avg_ranks), scale=st.sem(avg_ranks))\n",
    "    avg_avg_ranks = np.mean(avg_ranks)\n",
    "    mrr_mean = np.mean(mrrs)\n",
    "    mrr_std = np.std(mrrs)\n",
    "    return {\n",
    "        \"mrr_mean\": mrr_mean, \n",
    "        \"mrr_std\": mrr_std, \n",
    "        \"avg_rank\": avg_avg_ranks,\n",
    "        \"rank_ci\": ci\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa8f03d-73bb-4a4a-a9cd-9a06e70ecf9f",
   "metadata": {},
   "source": [
    "Put it all together and print the results! This will likely take a little while, and will print the MRR of each bootstrap as it goes, so you can see the progress. At the end you will see the summary we defined above, over all runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c6862c-634f-4b33-b186-13d532f8d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(searches, bootstraps=BOOTSTRAPS, sample_size=SAMPLE_SIZE):\n",
    "    bootstrap_results = []\n",
    "    print(\"--- STARTING evaluation runs: %s runs of %s queries each. ---\" % (bootstraps, sample_size))\n",
    "    for (i, bootstrap) in enumerate(select_searches(searches, bootstraps, sample_size)):\n",
    "        print(\"STARTING bootstrap run %s\" % i+1)\n",
    "        search_results = []\n",
    "        for search in bootstrap:\n",
    "            results = get_results_for_search(search)\n",
    "            page = int(search[\"query\"].get(\"page\", [0])[0])\n",
    "            per_page = int(search[\"query\"].get(\"per_page\", [DEFAULT_PER_PAGE])[0])\n",
    "            # When page > 1, pad the results with nan, so that the rank takes into account the pagination.\n",
    "            padded_results = ([np.nan] * (page * per_page)) + results\n",
    "            search_results.append({\"true\": search[\"documents\"][-1], \"returned\": padded_results, \"search\": search})\n",
    "            sleep(TIMEOUT)\n",
    "        mrr = mean_reciprocal_rank(search_results)\n",
    "        print(\"FINISHED run %s, mean reciprocal rank: %.2f\" % (i+1, mrr))\n",
    "        bootstrap_results.append({\"searches\": search_results, \"mrr\": mrr})\n",
    "    summary = summarise_results(bootstrap_results)\n",
    "    print(\"--- FINISHED evaluation runs ---\")\n",
    "    print(\"--- SUMMARY ---\")\n",
    "    print(\"* Bootstrap resampling runs: %s\" % bootstraps)\n",
    "    print(\"* Sample size: %s\" % sample_size)\n",
    "    print(\"* Mean Reciprocal Rank (Standard deviation): %.2f (%.2f)\" % (summary[\"mrr_mean\"], summary[\"mrr_std\"]))\n",
    "    print(\"* Maximum-likelihood rank for true result (Confidence interval): %.2f (%.2f, %.2f)\" % (summary[\"avg_rank\"], summary[\"rank_ci\"][0], summary[\"rank_ci\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854d8146-1bcc-4b99-bf85-e221b3733a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(searches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0af753-17ae-4602-8f1b-a977eb636f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19911ccf-f8c9-470e-9bb7-7bd3e311c7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
